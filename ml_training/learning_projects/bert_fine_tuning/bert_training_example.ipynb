{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_minimal_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObWnf7WE2Y0TIgReY74kNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcha-ultra/data_toolkit/blob/master/bert_minimal_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a minimal example of fine-tuning BERT to create a classifier for the [emotion dataset](https://huggingface.co/datasets/emotion).\n",
        "\n",
        "Adapted from https://colab.research.google.com/drive/18Qqox_QxJkOs80XVYaoLsdum0dX-Ilxb"
      ],
      "metadata": {
        "id": "PGT7gnX-ociD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "L-SaUqqvxbI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnFv-iMeoUbF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from os.path import join\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDRivaE1KYWA"
      },
      "source": [
        "# config info\n",
        "model_name = \"bert-base-uncased\"\n",
        "max_length = 512\n",
        "is_gpu = True\n",
        "\n",
        "# save info\n",
        "model_save_path = '/content/drive/MyDrive/ml_models'\n",
        "model_save_name = \"emotion-bert-base-uncased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFv-FiYtKuuf"
      },
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        " \n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed) \n",
        "\n",
        "set_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "# load and preprocess dataset\n",
        "emotion_dataset = load_dataset(\"emotion\")\n",
        "train_dataset = emotion_dataset['train'].map(lambda e: tokenizer(e['text'], truncation=True, padding=True, max_length=max_length), batched=True)\n",
        "valid_dataset = emotion_dataset['validation'].map(lambda e: tokenizer(e['text'], truncation=True, padding=True, max_length=max_length), batched=True)\n",
        "\n",
        "# set target names\n",
        "target_names = train_dataset.features['label'].names"
      ],
      "metadata": {
        "id": "9jufi-FWRFSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "def mk_bert_pt_classifier(model_name, target_names, is_gpu):\n",
        "  cpu_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names))\n",
        "  return cpu_model.to(\"cuda\") if is_gpu else cpu_model\n",
        "  # model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names)).to(\"cuda\")\n",
        "  # return model\n",
        "\n",
        "model = mk_bert_pt_classifier(model_name, target_names, is_gpu)"
      ],
      "metadata": {
        "id": "O8VtqyobAE5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the metrics that would be used for callback function to be passed to trainer constructor\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  accuracy = accuracy_score(labels, preds) # from `sklearn` package\n",
        "  return {\n",
        "      'accuracy': accuracy,\n",
        "  }\n",
        "\n",
        "# training arguments for trainer\n",
        "train_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "                                     # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=400,               # log & save weights each logging_steps\n",
        "    save_steps=400,\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "metadata": {
        "id": "Sd1hursL9HOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=train_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PfLsYFPcB4dw",
        "outputId": "6c60d063-a273-4e48-a456-d82bcfc1cb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running training *****\n",
            "  Num examples = 16000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6000/6000 18:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.231600</td>\n",
              "      <td>0.551792</td>\n",
              "      <td>0.830500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.469200</td>\n",
              "      <td>0.370523</td>\n",
              "      <td>0.912500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.322900</td>\n",
              "      <td>0.318753</td>\n",
              "      <td>0.924500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.231478</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.222400</td>\n",
              "      <td>0.205199</td>\n",
              "      <td>0.931500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.168000</td>\n",
              "      <td>0.208003</td>\n",
              "      <td>0.935000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.152800</td>\n",
              "      <td>0.176558</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.161500</td>\n",
              "      <td>0.203853</td>\n",
              "      <td>0.939500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.165400</td>\n",
              "      <td>0.172741</td>\n",
              "      <td>0.938500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.168449</td>\n",
              "      <td>0.935500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.117400</td>\n",
              "      <td>0.194463</td>\n",
              "      <td>0.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.108200</td>\n",
              "      <td>0.184393</td>\n",
              "      <td>0.939500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.102100</td>\n",
              "      <td>0.224325</td>\n",
              "      <td>0.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.090400</td>\n",
              "      <td>0.206073</td>\n",
              "      <td>0.944000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.119600</td>\n",
              "      <td>0.195136</td>\n",
              "      <td>0.939000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-400\n",
            "Configuration saved in ./results/checkpoint-400/config.json\n",
            "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-400/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-800\n",
            "Configuration saved in ./results/checkpoint-800/config.json\n",
            "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-800/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-800/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-1200\n",
            "Configuration saved in ./results/checkpoint-1200/config.json\n",
            "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-1200/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-1200/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-1600\n",
            "Configuration saved in ./results/checkpoint-1600/config.json\n",
            "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-1600/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-1600/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-2400\n",
            "Configuration saved in ./results/checkpoint-2400/config.json\n",
            "Model weights saved in ./results/checkpoint-2400/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-2400/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-2400/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-2800\n",
            "Configuration saved in ./results/checkpoint-2800/config.json\n",
            "Model weights saved in ./results/checkpoint-2800/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-2800/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-2800/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-3200\n",
            "Configuration saved in ./results/checkpoint-3200/config.json\n",
            "Model weights saved in ./results/checkpoint-3200/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-3200/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-3200/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-3600\n",
            "Configuration saved in ./results/checkpoint-3600/config.json\n",
            "Model weights saved in ./results/checkpoint-3600/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-3600/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-3600/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-4400\n",
            "Configuration saved in ./results/checkpoint-4400/config.json\n",
            "Model weights saved in ./results/checkpoint-4400/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-4400/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-4400/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-4800\n",
            "Configuration saved in ./results/checkpoint-4800/config.json\n",
            "Model weights saved in ./results/checkpoint-4800/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-4800/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-4800/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-5200\n",
            "Configuration saved in ./results/checkpoint-5200/config.json\n",
            "Model weights saved in ./results/checkpoint-5200/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-5200/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-5200/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-5600\n",
            "Configuration saved in ./results/checkpoint-5600/config.json\n",
            "Model weights saved in ./results/checkpoint-5600/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-5600/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-5600/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 20\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-4000 (score: 0.16844943165779114).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6000, training_loss=0.2587536163330078, metrics={'train_runtime': 1127.9173, 'train_samples_per_second': 42.556, 'train_steps_per_second': 5.32, 'total_flos': 1972745984977920.0, 'train_loss': 0.2587536163330078, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount gdrive\n",
        "drive.mount('/content/drive')\n",
        "save_path = join(model_save_path, model_save_name)"
      ],
      "metadata": {
        "id": "AZBCKKqedjXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the fine tuned model & tokenizer\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "IQ8lc5n-zFUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(save_path, num_labels=len(target_names)).to(\"cuda\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(save_path)"
      ],
      "metadata": {
        "id": "lsY7_7Cyc_bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text):\n",
        "    # prepare text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return target_names[probs.argmax()]"
      ],
      "metadata": {
        "id": "hVUyuI4ffvpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "text = \"\"\"\n",
        "This is amazing! I'm so happy.\n",
        "\"\"\"\n",
        "print(get_prediction(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp-1UNGHf1ux",
        "outputId": "a00f2ac5-89e4-404c-edfe-707a0abfb2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joy\n"
          ]
        }
      ]
    }
  ]
}
