
```{r libs}

pacman::p_load(caret, dataPreparation, magrittr, dplyr)
# library(caret)
# library(dataPreparation)
# library(magrittr)
# library(dplyr)

```

```{r ingest}

path <- "~/repos/toolkit/datasets/titanic/train.csv"
training_data <- read.csv(path)
seed <- 3000

```

## Introduction

This is a learning project for me to get acquainted with the basics of machine learning. As such, it is likely to contain mistakes, so please be aware of that if you intende to use this as a reference.

## Examination

Check the data to see if any data quality red flags stick out. The source of the data is from the Titanic ML competition in Kaggle: https://www.kaggle.com/c/titanic/data

```{r examination}

summary(training_data)
pointblank::scan_data(training_data)

```

Examination Notes:
* encoding required: `Ticket`, `Cabin`, `Embarked`
* some dimensional reduction is likely warranted; some of the character columns may not be predictive
* the only clear correlation is between `Pclass` and `Fare`, which are negatively correlated; they're also mildly correlated with `Survived`, the target variable, as expected
* `PassengerId` is just a unique identifier added by Kaggle can be dropped; `Name` is also unique and won't add value to prediction
* only missing data appears to be in `Age` and `Cabin`
* `Cabin` is super sparse and might not be that useful; consider dropping
* `Cabin` also has some values that are concatenations of multiple cabin numbers; may consider tokenizing these into different columns, or maybe converting to a "num_cabins" variable, but the column is so sparse it might not be worth it
* `Cabin` _also_ has some single-letter values that may need to be cleaned
* `Ticket` has high cardinality, with apparently arbitrary values and might not add much to prediction; consider dropping

## Cleaning

Clean out parts of the data that may cause problems for the model.

```{r cleaning}

clean_titanic_data <- function(df, has_Survived_col = TRUE) {
  data_cleaned <- subset(df, select = -c(PassengerId, Name))
  if (has_Survived_col)
    data_cleaned[["Survived"]] <- as.factor(data_cleaned[["Survived"]])
  return(data_cleaned)
}

training_data_cleaned <- clean_titanic_data(training_data)

```

## Encoding

Encode categorical variables into numerical ones so that they're usable by models. I decided to use target encoding due to its simplicity, so that I can understand what's happening.

```{r encoding}

# encode Sex column
encode_sex <- function(coldata) purrr::map_int(coldata, function(x) if(x == "male") 1L else 0L)

encode_titanic_data <- function(df) {
  training_data_encoded <- df
  training_data_encoded$Sex <- encode_sex(df$Sex)
  
  # training_data_encoded$Survived <- as.integer(as.character(training_data_encoded$Survived))
  
  # target encoding
  encoded_cols <- c("Ticket", "Cabin", "Embarked")
  encoding <- training_data_encoded %>% 
    mutate(Survived = as.integer(as.character(Survived))) %>% 
    build_target_encoding(cols_to_encode = encoded_cols, target_col = "Survived")
  training_data_encoded <- target_encode(training_data_encoded, target_encoding = encoding) %>% select(!all_of(encoded_cols))
  
  return(training_data_encoded)
}

training_data_encoded <- encode_titanic_data(training_data_cleaned)

```

I am rather skeptical as to how much `Ticket` and `Cabin` really contribute to the prediction, but perhaps we can do an analysis with and without them.

Another thing to note is that the blank (really NA) values of `Cabin` also got encoded; it's not clear to me whether that should have happened or not—I think that depends whether we're interpreting the NA values as meaningful or not... but `Cabin` itself is kind of meaningless, so it's all very muddy.

## Imputation

This is where we fill in NA values. I decided to use mean imputation, again for simplicity so that I can see what's happening. My understanding is that in a "real" project, single imputation is usually not a good idea.

```{r imputation}


impute_by_mean <- function(df, col_name) {
  df[[col_name]][is.na(df[[col_name]])] <- mean(df[[col_name]], na.rm=TRUE)
  return(df[[col_name]])
}

impute_titanic_data <- function(df) {
  col_to_impute <- "Age"
  training_data_imputed <- training_data_encoded
  training_data_imputed[[col_to_impute]] <- impute_by_mean(training_data_encoded, col_to_impute)
  return(training_data_imputed)
}

training_data_imputed <- impute_titanic_data(training_data_encoded)
```

## Dimensional Reduction

Try to perform dimensional reduction on the columns via PCA. One of the benefits of this is that it should reduce the chances of overfitting a model. This might not be necessary since I'll be doing cross-validation later on and in theory that should result in a model that doesn't overfit.

I'm also hoping to get some other things out of this:
1. confirm some predictions of mine:
   1a. `Cabin` and `Ticket` are just noise and don't add much to the predictive power of the data
   1b. `Fare` and `Pclass` will be squished together into a single component since they're so highly correlated
   1c. most of the explanatory power will be in `Fare/Pclass`, `Sex`, and `Age`, i.e. being higher class fare, female, and young will largely explain your survive or not, to the extent that survival is predictable in the data
2. get a good sense of what the principal components are and what they mean, conceptually

```{r dim_reduction_pca}

training_data_pca <- prcomp(training_data_imputed, center = TRUE,scale. = TRUE)
summary(training_data_pca)
print(training_data_pca)

```

### Prediction Results
* 1a. This is hard to tell from PCA and will have to wait until we train the model; certainly there are some relatively high coefficients in the components for those two variables, but that could happen in either case just as a part of the mathematics. I am encouraged that, at least up to PC7, there don't seem to be any components solely based on one or both of those variables, though.
* 1b. Largely borne out—almost all of the components up to PC7 that have high absolute values of `Pclass` or `Fare` coefficients will have a corresponding coefficient in the other variable of the opposite sign. However, there are caveats:
    + even when the signs are opposite, the actual magnitudes don't always match; e.g. PC6 has -0.16 coupled with 0.41; this might just be an artifact of how the components broke out, since we know the correlation isn't perfect
    + PC2 has a pretty negative `Pclass` coefficient but a `Fare` coefficient of almost 0, which seems odd and warrants further investigation
* 1c. The first two are corroborated somewhat by PC1 and PC3, but:
    + for `Pclass/Fare`, it's not as straightforward as "people with better tickets survive better"; instead it seems to specifically be _females_ with better tickets who survived well. Males with better tickets actually have a pretty negative survival coefficient. See below for more thoughts on that
    + the principal components don't reveal much about the relationship between `Age` and survival. In fact, there's no component that might represent a group concept of "children who survived"; upon reflection, this does make sense, because it seems quite likely that if you're going to allow a child into a life raft, you'd probably also allow at least one other person who is in their family that's older, which muddies the data and makes it hard for PCA to pick out that specific subgroup

### Speculation: Principal Component Group Interpretation

Mostly for fun, I will now perform some highly speculative interpretation of the characteristics represented by the principal components, by seeing if I can convert each one into a representation of a group, up to PC7 (the 90% variance threshold). I don't have hard evidence backing these interpretations, so it'll just be based on intuition—don't take these too seriously. Note that some of these groups might have 2 separate interpretations based on whether we are going in the positive or negative direction of the vector, but I tried to pick the one that made the most conceptual sense to me.

* `PC1`: these appear to be females with high-class tickets; out of all groups, these have the highest `Survived` coefficients; which does make sense in light of `PC3`
* `PC2`: older single males with high class tickets; no significant survival coefficient.
    + oddly enough these do not have a corresponding `Fare` coefficient to match the ticket class; not sure what's happening there, and may warrant further investigation
* `PC3`: adult males with high-class tickets that are part of a family; these have the lowest of the `Survived` coefficients out of all groups
    + it isn't clear why this is without digging further into the data, but speculating wildly, perhaps families with higher class tickets were also larger, and perhaps it's just less likely to for a big family to be able to get all its members on a life raft, and culture mores made it more likely for the adult male member to stay behind
    + it might be useful to calculate some sort of "family size" variable and see how well that correlates with survival
* `PC4`: older (or younger?) people that embarked in a particular location; perhaps some sort of travel group? No significant survival coefficient
* `PC5`: young males who didn't come with their parents; no significant survival coefficient
* `PC6`: older couples without children, on cheap fare; no significant survival coefficient
* `PC7`: women without children who came with sibling or spouse; no significant survival coefficient
    + given the strong `Sex` coefficient, my guess is that this is picking up some number of sisters going on the cruise together (male siblings and spouses should weaken the `Sex` coefficient, all else being equal)

## Model Training: KNN

Here we'll train the actual model, which I've arbitrarily chosen to be KNN. 

I'm actually going to try this first without any PCA transformation—I certainly gleaned some interesting results from looking at the basis vectors but want to see what happens by default without any transformations first.

```{r knn_training}

train_titanic_model <- function(training_df, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  # create prediction
  training_ctrl <- trainControl(method = "cv", number = 10)
  fit <- train(Survived ~ ., data = training_df, method = "knn", trControl=training_ctrl, preProcess = c("center", "scale"), tuneLength = 15)
  return(fit)
}

predict_titanic <- function(training_df, test_df, seed = NULL) {
  prediction <- predict(train_titanic_model(training_df), newdata = test_df)
  # temp1.1 <<- training_df # debug
  # temp1 <<- train_titanic_model(training_df, seed) # debug
  
  # examine prediction performance
  # print(test_df$Survived)
  if(!is.null(test_df[["Survived"]])) print(confusionMatrix(prediction, test_df[["Survived"]]))
  return(prediction)
}

predicted_Survived_training <- predict_titanic(training_data_imputed, training_data_imputed, seed = 3000)

```

So at first glance the results seem rather alarming. On the training data, the model is **98%** accurate. That seems astoundingly high for real data, given that I basically made random choices at each step of the training process, so I'm quite suspicious that we've overfitted here, with the high-cardinality, low-meaning variables `Cabin` and `Ticket` being the prime suspects.

Another thing I realized is that `Cabin` and `Ticket` have additional levels in the testing set that aren't in the training set, which makes the current encoding technique for those columns completely untenable; surely there must be ways to extract some information from these, but given my previous concerns about the lack of meaningful information here I'm simply going to drop these, and redo the training.

## Model Training: KNN (w/o `Cabin` and `Ticket`)

```{r}

training_df <- subset(training_data_imputed, select = -c(Survived_mean_by_Ticket, Survived_mean_by_Cabin))
predicted_Survived_training <- predict_titanic(training_df, training_df)

```

Somewhat encouragingly, dropping those two variables reduced the accuracy to a level that seems more consistent with the minimal model we've developed here (~85%), which lends credence to the idea that those two columns were causing overfitting.

I'll note here that I had expected cross-validation to deal with the overfitting issue; evidently that was not true in this case.

## Model Testing

Here we will prepare the testing data the same way that we did the training data, albeit streamlined.

```{r testing_data_prep}

testing_data <- read.csv("~/repos/toolkit/datasets/titanic/test.csv")

prep_testing_data <- function() {
testing_data_prepped <- subset(testing_data, select = -c(PassengerId, Name, Ticket, Cabin))
testing_data_prepped$Sex <- encode_sex(testing_data_prepped$Sex)
encoding_map <- data.frame(Embarked = training_data$Embarked, Survived_mean_by_Embarked = training_data_encoded$Survived_mean_by_Embarked) %>% unique()
testing_data_prepped$Survived_mean_by_Embarked <- purrr::map_dbl(testing_data_prepped$Embarked, function(emb_val) encoding_map$Survived_mean_by_Embarked[[which(encoding_map$Embarked == emb_val)]])
testing_data_prepped$Age <- impute_by_mean(testing_data_prepped, "Age")
testing_data_prepped$Fare <- impute_by_mean(testing_data_prepped, "Fare") # new; no NAs in training data
  return(testing_data_prepped)
}
testing_data_prepped <- prep_testing_data()

```

Running the actual test:

```{r testing_data_prediction}

predicted_Survived_testing <- predict_titanic(training_df, testing_data_prepped)
prediction_df <- data.frame(PassengerId = testing_data$PassengerId, Survived = predicted_Survived_testing)

# prediction <- predict(train_titanic_model(training_df), newdata = testing_data_prepped)
# prediction_df <- data.frame(PassengerId = testing_d/zata$PassengerId, Survived = prediction)

```

## Model Testing Results

The next step is to submit to Kaggle. While this is clearly not going to score high on the leaderboard, the hope here is that the model works about as well on the testing data as it did in the training data (~84%), which at least indicates that it's robust.

```{r save_prediction}

write.csv(prediction_df, "~/repos/toolkit/learning_projects/titanic/titanic_prediction_2.csv", row.names = FALSE)

```

[Update] After submitting the results, the success rate turned out to be 77.99%, which is... actually not terrible; the error rate difference is 7%, which intuitively seems okay as far as robustness goes. In terms of performance on the leaderboards, it's within the top quartile, which again is not bad for such a simple approach.

## Model Tweaking Ideas

Now that we know the performance of the initial version of the trained model is, it will be interesting to see what happens when we tweak the training process. I'll list the ideas I have so far below and try them out. Given that the model performs fairly well already, I expect that most of these won't result in significant better performance, or indeed even decrease performance, but in any case they should be informative.

* perform pca transform on data before training the model
* calculate "family size" and add it as a predictor
* use one hot encoding instead of target encoding
* try bayesian target encoding
* use multiple imputation instead of mean imputation
* extract some information from `Cabin` and `Ticket` columns and use that as predictors
* try repeated cross validation
* try a different model that's not knn

## Model Tweaking: Perform PCA transform on data before training

The robustness of the model seems okay-ish in my non-expert eyes (-7% from training to testing), but I want to see if I can improve it somewhat by dimensionality reduction, which, in theory, should reduce the amount of information clutter that could cause overfitting.

Since I already looked into PCA, I'm going to just use that as my reduction mechanism. First, we run PCA on the training predictors to see how many components to keep.


```{r pca_examination}

prep_training_data <- function(raw_data) {
  data_cleaned <- clean_titanic_data(raw_data, has_Survived_col = TRUE)
  data_encoded <- encode_titanic_data(data_cleaned)
  data_imputed <- impute_titanic_data(data_encoded)
  data_prepped <- subset(training_data_imputed, select = -c(Survived_mean_by_Ticket, Survived_mean_by_Cabin))
  return(data_prepped)
}
training_data_prepped <- prep_training_data(training_data)

pca_titanic_data <- function(data) {
  # data <- training_data_prepped # debug
  training_data_predictors <- select(data, all_of(setdiff(colnames(data), "Survived")))
  predictors_pca <- prcomp(training_data_predictors, center = TRUE, scale. = TRUE)
  return(predictors_pca)
}
training_data_pca <- pca_titanic_data(training_data_prepped)

summary(training_data_pca)
```

I'm going to drop the last 2 components (PC6 and PC7); I'm not sure if that's too many, but it still leaves me with ~87% variance, so it seems okay.

### Dim Reduction Result Forecasts

It should be interesting to make some forecasts on what the results might look like and see how they match up:
1. Running the prediction on the dim-reduced _training_ data should result in lower predictive ability, compared to the 85% it is on the non-reduced data. This is because the model now has less information to use for prediction, which means the fit won't be as good.
2. The predictive ability on the _testing_ data should NOT decrease, which means the robustness should increase, because the predictive abilities on the testing data is now closer to that on the training data.
3. There might actually be a small increase on the predictive ability on the testing data, but I'm less sure about this than for #2. I'd definitely still lean towards yes, though, because again, if you're throwing away small variations in the data that's very specific to the training set, you _should_ then improve your predictive ability overall due to not having the model be dependent on those small variations. 
   * I don't think this will be a large improvement, though, because the model wasn't super brittle pre-dim-reduction. Assuming prediction #1 holds, that means there's a cap on how good it can be, since it would be unlikely for the model to work better on the testing data than in the training data. I would be surprised (and quite happy) if it even improves to something like 81%.



```{r predict_pca_training}

rotate_training_data <- function(data, data_pca, drop = 6:7) {
  rotated_predictors <- data_pca$x
  rotated_predictors_drop <- rotated_predictors[, -drop]
  training_data_pca_rotated <- cbind(as.data.frame(rotated_predictors_drop), Survived = data$Survived)
  return(training_data_pca_rotated)
}
training_data_rotated <- rotate_training_data(training_data_prepped, training_data_pca)

predicted_Survived_training <- predict_titanic(training_data_rotated, training_data_rotated, seed = seed)

```

Running the prediction on the dim-reduced training data did indeed decrease the predictive ability to about 82.5%, per forcast #1, but happily, it's quite some ways from the previous 78% predictive ability on the non-reduced training data. That means there's some room for that to improve.


```{r predict_pca_testing}

prep_testing_data_2 <- function(raw_data, training_data_encoded) {
  testing_data_prepped <- subset(raw_data, select = -c(PassengerId, Name, Ticket, Cabin))
  testing_data_prepped$Sex <- encode_sex(testing_data_prepped$Sex)
  encoding_map <- data.frame(Embarked = training_data$Embarked, Survived_mean_by_Embarked = training_data_encoded$Survived_mean_by_Embarked) %>% unique()
  testing_data_prepped$Survived_mean_by_Embarked <- purrr::map_dbl(testing_data_prepped$Embarked, function(emb_val) encoding_map$Survived_mean_by_Embarked[[which(encoding_map$Embarked == emb_val)]])
  testing_data_prepped$Age <- impute_by_mean(testing_data_prepped, "Age")
  testing_data_prepped$Fare <- impute_by_mean(testing_data_prepped, "Fare") # new; no NAs in training data
  return(testing_data_prepped)
}
testing_data_prepped <- prep_testing_data_2(testing_data, training_data_encoded)

rotate_testing_data <- function(data, data_pca, drop = 6:7) {
  # data_pca <- training_data_pca
  # data <- testing_data_prepped
  rotated_predictors <- predict(data_pca, newdata = data)
  rotated_predictors_drop <- rotated_predictors[, -drop]
  testing_data_pca_rotated <- as.data.frame(rotated_predictors_drop)
  return(testing_data_pca_rotated)
}
testing_data_rotated <- rotate_testing_data(testing_data_prepped, training_data_pca)

predicted_Survived_testing <- predict_titanic(training_data_rotated, testing_data_rotated, seed = seed)

write_titanic_prediction <- function(prediction, name) {
  prediction_df <- data.frame(PassengerId = testing_data$PassengerId, Survived = prediction)
  write.csv(prediction_df, paste0("~/repos/toolkit/learning_projects/titanic/", name), row.names = FALSE)
}
write_titanic_prediction(predicted_Survived_testing, "titanic_prediction_pca.csv")
```

Submitting the results to Kaggle, it appears that the predictive ability has now improved to 0.78947, which is about 1% better than before. Not as high as I'd hoped, but at least it's not lower. Apparently, this 1% increase was enough to jump to within the 10th percentile (1306) on the leaderboards, which doesn't give me much confidence in the leaderboards—I'm assuming there's a morass of predictions that in the 78-79% range.

Regardless, the robustness is now all the way up to -3.5%, from -7%, which seems quite good for a relatively simple transformation.

## Thoughts

The following are thoughts on what I've learned from the project so far. As with the other parts of this document, take these with a grain of salt, since there is a great deal about this topic that I don't know about yet.

* Overfitting really is a big problem that seems to pop up really easily.
    + can help with this by dropping variables that aren't likely to have any predictive power
    + dimensional reduction (such as via PCA) also helps here
* When cleaning the data, it's important to check for values that might code for missing data but wasn't recognized as such by the data package, such as blank strings, and clean those into the appropriate representation in the data. I forgot to do this so ended up thinking initially that there was very little data missing when I was looking at the summary of the data
* PCA is as great as I thought it would be, even if reading the components can get a little messy. Personally, I find it kind of magical, even though I know how it works mechanically and on a conceptual level it's incredibly simple.
* High-cardinality categorical variables are tricky:
    + they are rather difficult to encode, since there may be values in the test (or indeed real-world) datasets that don't occur in the training dataset; that seems like it could lead to brittle models that get completely flummoxed with input from real data they haven't seen before
    + they seem to easily lead to overfitting; I'm guessing this has something to do with there not being enough observations for some values, which could lead to models picking up coincidental relationships in small samples
    + besides just dropping these variables, it might be possible to extract some sort of numerical information from them and use that as a predictor