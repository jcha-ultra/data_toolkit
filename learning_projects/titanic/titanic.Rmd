
```{r libs}

library(caret)
library(dataPreparation)
library(magrittr)
library(dplyr)

```

```{r ingest}

path <- "~/repos/toolkit/datasets/titanic/train.csv"
training_data <- read.csv(path)

```

## Introduction

This is a learning project for me to get acquainted with the basics of machine learning. As such, it is likely to contain mistakes, so please be aware of that if you intende to use this as a reference.

## Examination

Check the data to see if any data quality red flags stick out. The source of the data is from the Titanic ML competition in Kaggle: https://www.kaggle.com/c/titanic/data

```{r examination}

summary(training_data)
pointblank::scan_data(training_data)

```

Examination Notes:
* encoding required: `Ticket`, `Cabin`, `Embarked`
* some dimensional reduction is likely warranted; some of the character columns may not be predictive
* the only clear correlation is between `Pclass` and `Fare`, which are negatively correlated; they're also mildly correlated with `Survived`, the target variable, as expected
* `PassengerId` is just a unique identifier added by Kaggle can be dropped; `Name` is also unique and won't add value to prediction
* only missing data appears to be in `Age` and `Cabin`
* `Cabin` is super sparse and might not be that useful; consider dropping
* `Cabin` also has some values that are concatenations of multiple cabin numbers; may consider tokenizing these into different columns, or maybe converting to a "num_cabins" variable, but the column is so sparse it might not be worth it
* `Cabin` _also_ has some single-letter values that may need to be cleaned
* `Ticket` has high cardinality, with apparently arbitrary values and might not add much to prediction; consider dropping

## Cleaning

Clean out parts of the data that may cause problems for the model.

```{r cleaning}

training_data_cleaned <- subset(training_data, select = -c(PassengerId, Name))

```

## Encoding

Encode categorical variables into numerical ones so that they're usable by models. I decided to use target encoding due to its simplicity, so that I can understand what's happening.

```{r encoding}

# encode Sex column
encode_sex <- function(coldata) purrr::map_int(coldata, function(x) if(x == "male") 1L else 0L)
training_data_encoded <- training_data_cleaned
# training_data_encoded$Sex <- purrr::map_int(training_data_cleaned$Sex, function(x) if(x == "male") 1L else 0L)
training_data_encoded$Sex <- encode_sex(training_data_cleaned$Sex)

# target encoding
encoded_cols <- c("Ticket", "Cabin", "Embarked")
encoding <- build_target_encoding(training_data_encoded, cols_to_encode = encoded_cols, target_col = "Survived")
training_data_encoded <- target_encode(training_data_encoded, target_encoding = encoding) %>% select(!all_of(encoded_cols))

```

I am rather skeptical as to how much `Ticket` and `Cabin` really contribute to the prediction, but perhaps we can do an analysis with and without them.

Another thing to note is that the blank (really NA) values of `Cabin` also got encoded; it's not clear to me whether that should have happened or not—I think that depends whether we're interpreting the NA values as meaningful or not... but `Cabin` itself is kind of meaningless, so it's all very muddy.

## Imputation

This is where we fill in NA values. I decided to use mean imputation, again for simplicity so that I can see what's happening. My understanding is that in a "real" project, single imputation is usually not a good idea.

```{r imputation}

col_to_impute <- "Age"

impute_by_mean <- function(df, col_name) {
  df[[col_name]][is.na(df[[col_name]])] <- mean(df[[col_name]], na.rm=TRUE)
  return(df[[col_name]])
}

training_data_imputed <- training_data_encoded
training_data_imputed[[col_to_impute]] <- impute_by_mean(training_data_encoded, col_to_impute)
```

## Dimensional Reduction

Try to perform dimensional reduction on the columns via PCA. One of the benefits of this is that it should reduce the chances of overfitting a model. This might not be necessary since I'll be doing cross-validation later on and in theory that should result in a model that doesn't overfit.

I'm also hoping to get some other things out of this:
1. confirm some predictions of mine:
   1a. `Cabin` and `Ticket` are just noise and don't add much to the predictive power of the data
   1b. `Fare` and `Pclass` will be squished together into a single component since they're so highly correlated
   1c. most of the explanatory power will be in `Fare/Pclass`, `Sex`, and `Age`, i.e. being higher class fare, female, and young will largely explain your survive or not, to the extent that survival is predictable in the data
2. get a good sense of what the principal components are and what they mean, conceptually

```{r dim_reduction_pca}

training_data_pca <- prcomp(training_data_imputed, center = TRUE,scale. = TRUE)
summary(training_data_pca)
print(training_data_pca)

```

### Prediction Results
* 1a. This is hard to tell from PCA and will have to wait until we train the model; certainly there are some relatively high coefficients in the components for those two variables, but that could happen in either case just as a part of the mathematics. I am encouraged that, at least up to PC7, there don't seem to be any components solely based on one or both of those variables, though.
* 1b. Largely borne out—almost all of the components up to PC7 that have high absolute values of `Pclass` or `Fare` coefficients will have a corresponding coefficient in the other variable of the opposite sign. However, there are caveats:
    + even when the signs are opposite, the actual magnitudes don't always match; e.g. PC6 has -0.16 coupled with 0.41; this might just be an artifact of how the components broke out, since we know the correlation isn't perfect
    + PC2 has a pretty negative `Pclass` coefficient but a `Fare` coefficient of almost 0, which seems odd and warrants further investigation
* 1c. The first two are corroborated somewhat by PC1 and PC3, but:
    + for `Pclass/Fare`, it's not as straightforward as "people with better tickets survive better"; instead it seems to specifically be _females_ with better tickets who survived well. Males with better tickets actually have a pretty negative survival coefficient. See below for more thoughts on that
    + the principal components don't reveal much about the relationship between `Age` and survival. In fact, there's no component that might represent a group concept of "children who survived"; upon reflection, this does make sense, because it seems quite likely that if you're going to allow a child into a life raft, you'd probably also allow at least one other person who is in their family that's older, which muddies the data and makes it hard for PCA to pick out that specific subgroup

### Speculation: Principal Component Group Interpretation

Mostly for fun, I will now perform some highly speculative interpretation of the characteristics represented by the principal components, by seeing if I can convert each one into a representation of a group, up to PC7 (the 90% variance threshold). I don't have hard evidence backing these interpretations, so it'll just be based on intuition—don't take these too seriously. Note that some of these groups might have 2 separate interpretations based on whether we are going in the positive or negative direction of the vector, but I tried to pick the one that made the most conceptual sense to me.

* `PC1`: these appear to be females with high-class tickets; out of all groups, these have the highest `Survived` coefficients; which does make sense in light of `PC3`
* `PC2`: older single males with high class tickets; no significant survival coefficient.
    + oddly enough these do not have a corresponding `Fare` coefficient to match the ticket class; not sure what's happening there, and may warrant further investigation
* `PC3`: adult males with high-class tickets that are part of a family; these have the lowest of the `Survived` coefficients out of all groups
    + it isn't clear why this is without digging further into the data, but speculating wildly, perhaps families with higher class tickets were also larger, and perhaps it's just less likely to for a big family to be able to get all its members on a life raft, and culture mores made it more likely for the adult male member to stay behind
    + it might be useful to calculate some sort of "family size" variable and see how well that correlates with survival
* `PC4`: older (or younger?) people that embarked in a particular location; perhaps some sort of travel group? No significant survival coefficient
* `PC5`: young males who didn't come with their parents; no significant survival coefficient
* `PC6`: older couples without children, on cheap fare; no significant survival coefficient
* `PC7`: women without children who came with sibling or spouse; no significant survival coefficient
    + given the strong `Sex` coefficient, my guess is that this is picking up some number of sisters going on the cruise together (male siblings and spouses should weaken the `Sex` coefficient, all else being equal)