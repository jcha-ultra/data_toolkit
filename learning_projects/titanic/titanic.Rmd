
```{r libs}

library(caret)
library(dataPreparation)
library(magrittr)
library(dplyr)

```

```{r ingest}

path <- "~/repos/toolkit/datasets/titanic/train.csv"
training_data <- read.csv(path)

```

## Introduction

This is a learning project for me to get acquainted with the basics of machine learning. As such, it is likely to contain mistakes, so please be aware of that if you intende to use this as a reference.

## Examination

Check the data to see if any data quality red flags stick out. The source of the data is from the Titanic ML competition in Kaggle: https://www.kaggle.com/c/titanic/data

```{r examination}

summary(training_data)
pointblank::scan_data(training_data)

```

Examination Notes:
* encoding required: `Ticket`, `Cabin`, `Embarked`
* some dimensional reduction is likely warranted; some of the character columns may not be predictive
* the only clear correlation is between `Pclass` and `Fare`, which are negatively correlated; they're also mildly correlated with `Survived`, the target variable, as expected
* `PassengerId` is just a unique identifier added by Kaggle can be dropped; `Name` is also unique and won't add value to prediction
* only missing data appears to be in `Age` and `Cabin`
* `Cabin` is super sparse and might not be that useful; consider dropping
* `Cabin` also has some values that are concatenations of multiple cabin numbers; may consider tokenizing these into different columns, or maybe converting to a "num_cabins" variable, but the column is so sparse it might not be worth it
* `Cabin` _also_ has some single-letter values that may need to be cleaned
* `Ticket` has high cardinality, with apparently arbitrary values and might not add much to prediction; consider dropping

## Cleaning

Clean out parts of the data that may cause problems for the model.

```{r cleaning}

training_data_cleaned <- subset(training_data, select = -c(PassengerId, Name))

```

## Encoding

Encode categorical variables into numerical ones so that they're usable by models. I decided to use target encoding due to its simplicity, so that I can understand what's happening.

```{r encoding}

# encode Sex column
training_data_encoded <- training_data_cleaned
training_data_encoded$Sex <- purrr::map_int(training_data_cleaned$Sex, function(x) if(x == "male") 1L else 0L)

# target encoding
encoded_cols <- c("Ticket", "Cabin", "Embarked")
encoding <- build_target_encoding(training_data_encoded, cols_to_encode = encoded_cols, target_col = "Survived")
training_data_encoded <- target_encode(training_data_encoded, target_encoding = encoding) %>% select(!all_of(encoded_cols))

```

I am rather skeptical as to how much `Ticket` and `Cabin` really contribute to the prediction, but perhaps we can do an analysis with and without them.

Another thing to note is that the blank (really NA) values of `Cabin` also got encoded; it's not clear to me whether that should have happened or notâ€”I think that depends whether we're interpreting the NA values as meaningful or not... but `Cabin` itself is kind of meaningless, so it's all very muddy.

## Imputation

This is where we fill in NA values. I decided to use mean imputation, again for simplicity so that I can see what's happening. My understanding is that in a "real" project, single imputation is usually not a good idea.

```{r imputation}

col_to_impute <- "Age"

impute_by_mean <- function(df, col_name) {
  df[[col_name]][is.na(df[[col_name]])] <- mean(df[[col_name]], na.rm=TRUE)
  return(df[[col_name]])
}

training_data_imputed <- training_data_encoded
training_data_imputed[[col_to_impute]] <- impute_by_mean(training_data_encoded, col_to_impute)

```

## Dimensional Reduction

Try to perform dimensional reduction on the columns via PCA. One of the benefits of this is that it should reduce the chances of overfitting a model. This might not be necessary since I'll be doing cross-validation later on and in theory that should result in a model that doesn't overfit.

I'm also hoping to get some other things out of this:
1. confirm some predictions of mine:
   1a. `Cabin` and `Ticket` are just noise and don't add much to the predictive power of the data
   1b. `Fare` and `Pclass` will be squished together into a single component since they're so highly correlated
   1c. most of the explanatory power will be in `Fare/Pclass`, `Sex`, and `Age`, i.e. being higher class fare, female, and young will largely explain your survive or not, to the extent that survival is predictable in the data
2. get a good sense of what the principal components are and what they mean, conceptually