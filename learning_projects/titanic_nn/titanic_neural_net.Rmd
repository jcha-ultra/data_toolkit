# Neural Net Exploration

## Introduction

This project will be an exploration of machine learning using neural nets. I'll be using the same Titanic dataset I used for the other exploration project in `titanic/titanic.Rmd`, starting with the post-PCA transform version. Even more than that other one, I'll be taking an experimental approach here, recording all the things I tried, including the mistakes and false paths I go down. My hope is that this will serve as a chronicle of someone's initial foray into this topic.

Please note that this is not the work of an expert, and any conclusions and procedures shown here may be incorrect.

## Overall Expectations

I'd like to start off with what my expectations on what I will find out with this exploration. From what I've read so far, compared to the individual linear models often used in machine learning, neural nets would seem to be applicable to a wider range of problems, but are both more computationally expensive and more of a black box.

I think the compute requirement has turned out to be a strength rather than a weakness (at least for complex problems), because the flip side of it is that (deep) neural nets also scale way better with additional compute, which we are continually getting more of. Being a black box is a much bigger problem, because it greatly hinders the interpretability of the system, and makes it way harder to fix alignment problems that will inevitably pop up.

With regards to my specific effort on the Titanic dataset, I have my doubts as to how much better the neural net can be compared to the previous model I tried (KNN). I think realistically, there's a limit to how predictive the information we're given can truly be for the survival—surely there are factors outside of what's recorded in the dataset that contributed to whether someone survived or not. With that said, maybe there is something that the neural net can pick up that the other model couldn't; I would be quite happy if it got to 90% accuracy on the testing set.

## Initial Prep

We'll prep the data the same way we did earlier.

```{r data_prep}

pacman::p_load(caret, dataPreparation, magrittr, dplyr)
set.seed(3000)

# recreating data prep from titanic.Rmd
source("~/repos/toolkit/learning_projects/titanic/titanic_functions.R")
training_data <- read.csv("~/repos/toolkit/datasets/titanic/train.csv")
testing_data <- read.csv("~/repos/toolkit/datasets/titanic/test.csv")
training_data_prepped <- prep_training_data(training_data)
training_data_pca <- pca_titanic_data(training_data_prepped)
training_data_rotated <- rotate_training_data(training_data_prepped, training_data_pca)

training_data_cleaned <- clean_titanic_data(training_data)
training_data_encoded <- encode_titanic_data(training_data_cleaned)
testing_data_prepped <- prep_testing_data_2(testing_data, training_data_encoded)
testing_data_rotated <- rotate_testing_data(testing_data_prepped, training_data_pca)

```

## Initial Test

Before trying the Titanic dataset, I want to do an initial test on a very simple example, one that I expect the neural net to have no trouble with even with largely default settings: predicting the roots of some positive numbers.

```{r roots_test}

library("neuralnet")
training_data <- read.csv("~/repos/toolkit/datasets/roots.csv", header=TRUE)
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

It's... not horrible? At least it follows the shape of the expected values. But it's not great either. The reason is almost certainly that the default number of neurons is just 1. So, if we increase that...
```{r}
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data, hidden = 10)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

Much better. Now let's see how well it interpolates and extrapolates; I expect the standard results for that—interpolation results to be largely comparable to the training set, and extrapolation to be okay at first and get worse as it gets further away.
```{r roots_test_interpolation}

interpolation_test_data <- 1:49 + .5
extrapolation_test_data <- 51:100

predict_root <- function(k_vec, model) {
  data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
    mutate(nn_output = predict(model, select(., k))[,1]) %>%
    mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
roots_interpolation_outputs <- predict_root(interpolation_test_data, simple_test_model)
print(roots_interpolation_outputs)
```

That looks fine, as expected. For extrapolation:

```{r roots_test_extrapolation}

roots_extrapolation_outputs <- predict_root(extrapolation_test_data, simple_test_model)
print(roots_extrapolation_outputs)
plot(roots_extrapolation_outputs$k, roots_extrapolation_outputs$diff_pct)
```

Again, no surprises. It's interesting that the error percentage goes up essentially linearly as `k` gets larger, which means that the error term must have been `O(k^2)`. It seems like there should be a fairly straightforward mathematical proof of that, though I think I'm going to skip over that derivation to get to the main course here.

## Titanic Prediction via NN (first try)

I don't think this will work too well, but I'm going to first try straight up using the exact training parameters as I did with the previous section, except now with the Titanic dataset. It _shouldn't_ work, because the previous output wasn't for classification, while this one definitely is.

```{r titanic_nn_1}

titanic_model_1 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10)

compile_titanic_output_1 <- function(predictors, expected, nn_output) {
  predictors %>% cbind(expected) %>%
    mutate(nn_output = nn_output[,2])
  # data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
  # mutate(nn_output = predict(model, select(., k))[,1]) %>%
  # mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
titanic_training_output_1 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_1$net.result[[1]])

print(titanic_training_output_1)

```

This turned out better than I expected, though I'm fairly certain the loss minimization took way longer than it should have to converge (about 50k steps). The results did indeed end up not being categorical, but they still roughly corresponded to the expected results (i.e. the lower values will tend to correspond to `Survived` being 0, and vice versa for higher values).

Clearly I need to do some research on how to turn this into a classification result, but for the sake of getting some sort of result, I'm simply going to round the result to 0 or 1 and see how it does.

```{r titanic_nn_1_verification}

correct_pct <- titanic_training_output_1 %>% mutate(prediction = round(nn_output)) %>%
  filter(as.integer(as.character(Survived)) - prediction == 0) %>%
  nrow() %>% `/`(nrow(titanic_training_output_1))
print(correct_pct)
```

It's... about 85.86%??? This seems absolutely incredible to me, because I have no clue if the neural net parameters are appropriate for this task, and I basically made up the mapping to the binary result without any consultation as to what the best thing to do there is. And yet we still ended up about 4% better than the previous model.

With that said, there's still a good chance that this model is not robust and flounders on the test set, so we should confirm that.

```{r testing_output_1}

titanic_testing_output_1 <- predict(titanic_model_1, testing_data_rotated) %>%
  compile_titanic_output_1(testing_data_rotated, rep(NA, nrow(testing_data_rotated)), nn_output = .)

write_titanic_prediction(prediction = round(titanic_testing_output_1$nn_output), testing_data = testing_data, name = "titanic_prediction_nn_1.csv", dir = "~/repos/toolkit/learning_projects/titanic_nn")
```

Okay, so we got 76.32%, which is _not_ an improvement. Clearly, the model we trained was quite brittle. I wonder if the test dataset required too much extrapolation, or if the model also isn't that good with interpolation, either? In any case, we'll put the finer adjustments to the side for now, and fix the major issues first.

So, what are those major (potential) issues?
1. We clearly need a better system of mapping from the numerical output to the prediction categories. I'm sure there's a standard way of doing this, so this should just be a quick research stint.
2. We're using a 1/10/1 structure. Perhaps changing it around will give better results?
3. I'm pretty sure the default loss function is mean-squared. That might not be appropriate for a binary classifier.
4. I think by default the `neuralnet` trainer doesn't have an activation function enabled, so maybe we were pretty much just doing linear regression?

Hopefully, by looking into these, I can both make the loss function converge faster, and make the model more robust.