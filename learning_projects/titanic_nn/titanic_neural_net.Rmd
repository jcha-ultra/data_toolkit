# Neural Net Exploration

## Introduction

This project will be an exploration of machine learning using neural nets. I'll be using the same Titanic dataset I used for the other exploration project in `titanic/titanic.Rmd`, starting with the post-PCA transform version. Even more than that other one, I'll be taking an experimental approach here, recording all the things I tried, including the mistakes and false paths I go down. My hope is that this will serve as a chronicle of someone's initial foray into this topic.

Please note that this is not the work of an expert, and any conclusions and procedures shown here may be incorrect.

## Overall Expectations

I'd like to start off with what my expectations on what I will find out with this exploration. From what I've read so far, compared to the individual linear models often used in machine learning, neural nets would seem to be applicable to a wider range of problems, but are both more computationally expensive and more of a black box.

I think the compute requirement has turned out to be a strength rather than a weakness (at least for complex problems), because the flip side of it is that (deep) neural nets also scale way better with additional compute, which we are continually getting more of. Being a black box is a much bigger problem, because it greatly hinders the interpretability of the system, and makes it way harder to fix alignment problems that will inevitably pop up.

With regards to my specific effort on the Titanic dataset, I have my doubts as to how much better the neural net can be compared to the previous model I tried (KNN). I think realistically, there's a limit to how predictive the information we're given can truly be for the survival—surely there are factors outside of what's recorded in the dataset that contributed to whether someone survived or not. With that said, maybe there is something that the neural net can pick up that the other model couldn't; I would be quite happy if it got to 90% accuracy on the testing set.

## Initial Prep

We'll prep the data the same way we did earlier.

```{r data_prep}

pacman::p_load(caret, dataPreparation, magrittr, dplyr)
set.seed(3000)

# recreating data prep from titanic.Rmd
source("~/repos/toolkit/learning_projects/titanic/titanic_functions.R")
training_data <- read.csv("~/repos/toolkit/datasets/titanic/train.csv")
testing_data <- read.csv("~/repos/toolkit/datasets/titanic/test.csv")
training_data_prepped <- prep_training_data(training_data)
training_data_pca <- pca_titanic_data(training_data_prepped)
training_data_rotated <- rotate_training_data(training_data_prepped, training_data_pca)

training_data_cleaned <- clean_titanic_data(training_data)
training_data_encoded <- encode_titanic_data(training_data_cleaned)
testing_data_prepped <- prep_testing_data_2(testing_data, training_data_encoded)
testing_data_rotated <- rotate_testing_data(testing_data_prepped, training_data_pca)

```

## Initial Test

Before trying the Titanic dataset, I want to do an initial test on a very simple example, one that I expect the neural net to have no trouble with even with largely default settings: predicting the roots of some positive numbers.

```{r roots_test}

library("neuralnet")
training_data <- read.csv("~/repos/toolkit/datasets/roots.csv", header=TRUE)
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

It's... not horrible? At least it follows the shape of the expected values. But it's not great either. The reason is almost certainly that the default number of neurons is just 1. So, if we increase that...
```{r}
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data, hidden = 10)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

Much better. Now let's see how well it interpolates and extrapolates; I expect the standard results for that—interpolation results to be largely comparable to the training set, and extrapolation to be okay at first and get worse as it gets further away.
```{r roots_test_interpolation}

interpolation_test_data <- 1:49 + .5
extrapolation_test_data <- 51:100

predict_root <- function(k_vec, model) {
  data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
    mutate(nn_output = predict(model, select(., k))[,1]) %>%
    mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
roots_interpolation_outputs <- predict_root(interpolation_test_data, simple_test_model)
print(roots_interpolation_outputs)
```

That looks fine, as expected. For extrapolation:

```{r roots_test_extrapolation}

roots_extrapolation_outputs <- predict_root(extrapolation_test_data, simple_test_model)
print(roots_extrapolation_outputs)
plot(roots_extrapolation_outputs$k, roots_extrapolation_outputs$diff_pct)
```

Again, no surprises. It's interesting that the error percentage goes up essentially linearly as `k` gets larger, which means that the error term must have been `O(k^2)`. It seems like there should be a fairly straightforward mathematical proof of that, though I think I'm going to skip over that derivation to get to the main course here.

## Titanic Prediction via NN (first try)

I don't think this will work too well, but I'm going to first try straight up using the exact training parameters as I did with the previous section, except now with the Titanic dataset. It _shouldn't_ work, because the previous output wasn't for classification, while this one definitely is.

```{r titanic_nn_1}

titanic_model_1 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10)

compile_titanic_output_1 <- function(predictors, expected, nn_output) {
  predictors %>% cbind(expected) %>%
    mutate(nn_output = nn_output[,2])
  # data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
  # mutate(nn_output = predict(model, select(., k))[,1]) %>%
  # mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
titanic_training_output_1 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_1$net.result[[1]])

print(titanic_training_output_1)

compile_titanic_output_2 <- function(predictors, expected, nn_output, output_vec = 2) {
  predictors %>% cbind(expected) %>%
    mutate(nn_output = nn_output[,output_vec])
  # data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
  # mutate(nn_output = predict(model, select(., k))[,1]) %>%
  # mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
```

This turned out better than I expected, though I'm fairly certain the loss minimization took way longer than it should have to converge (about 50k steps). The results did indeed end up not being categorical, but they still roughly corresponded to the expected results (i.e. the lower values will tend to correspond to `Survived` being 0, and vice versa for higher values).

Clearly I need to do some research on how to turn this into a classification result, but for the sake of getting some sort of result, I'm simply going to round the result to 0 or 1 and see how it does.

```{r titanic_nn_1_verification}

correct_pct <- titanic_training_output_1 %>% mutate(prediction = round(nn_output)) %>%
  filter(as.integer(as.character(Survived)) - prediction == 0) %>%
  nrow() %>% `/`(nrow(titanic_training_output_1))
print(correct_pct)
```

It's... about 85.86%??? This seems absolutely incredible to me, because I have no clue if the neural net parameters are appropriate for this task, and I basically made up the mapping to the binary result without any consultation as to what the best thing to do there is. And yet we still ended up about 4% better than the previous model.

With that said, there's still a good chance that this model is not robust and flounders on the test set, so we should confirm that.

```{r testing_output_1}

titanic_testing_output_1 <- predict(titanic_model_1, testing_data_rotated) %>%
  compile_titanic_output_1(testing_data_rotated, rep(NA, nrow(testing_data_rotated)), nn_output = .)

write_titanic_prediction(prediction = round(titanic_testing_output_1$nn_output), testing_data = testing_data, name = "titanic_prediction_nn_1.csv", dir = "~/repos/toolkit/learning_projects/titanic_nn")
```

Okay, so we got 76.32%, which is _not_ an improvement. Clearly, the model we trained was quite brittle. I wonder if the test dataset required too much extrapolation, or if the model also isn't that good with interpolation, either? In any case, we'll put the finer adjustments to the side for now, and fix the major issues first.

So, what are those major (potential) issues?
1. We clearly need a better system of mapping from the numerical output to the prediction categories. I'm sure there's a standard way of doing this, so this should just be a quick research stint.
2. We're using a 1/10/1 structure. Perhaps changing it around will give better results?
3. I'm pretty sure the default loss function is mean-squared. That might not be appropriate for a binary classifier.
4. I think by default the `neuralnet` trainer doesn't have an activation function enabled, so maybe we were pretty much just doing linear regression?

Hopefully, by looking into these, I can both make the loss function converge faster, and make the model more robust.

## Titanic Prediction via NN (second try)

Okay, so here I have to make it clear that, even more than before, I am now firmly beyond my own current knowledge on the subject at this point. For the sake of my own record I'm still going to write down all the things I try, but I'm going to be making a lot of missteps here. Tread lightly, readers.

In any case, the first thing I'm going to try is to switch the loss function away from squared errors. The reason is that we're doing a binary classification, which I think SSE isn't optimized for; I believe the right loss function is "cross-entropy", from what I've read. So let's try that...

```{r model_2.1}

titanic_model_2 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10, err.fct = "ce", lifesign = "minimal")

```

...It didn't work. This is the error I got:

`hidden: 10    thresh: 0.01    rep: 1/1    steps: Warning in log(x) : NaNs produced`
`Warning in log(x) : NaNs produced`
`Warning in log(1 - x) : NaNs produced`
`Warning: 'err.fct' does not fit 'data' or 'act.fct'`
`  81034	error: NaN      	time: 59.3 secs`
  
I think I know why; the current output from the model is not actually constrained within 0 and 1, which obviously would cause `NaNs` when passed through the `log` function as part of cross-entropy. And the reason the output is not so constrained is (I think) that we don't have an activation function specified, so the node connection weights are just being passed straight through.

I'm going to try enabling the activation function (which by default is logistical regression).

```{r model_2.2}

titanic_model_2 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10, linear.output = FALSE, err.fct = "ce", lifesign = "minimal")
```

It worked! Or at least, there's not an error. On reading the `neuralnet` docs, I notice that I was wrong about the activation function not being enabled—it's more that it just wasn't applied to the final output, which `linear.output = FALSE` enables.

I have a feeling that it didn't actually work that well though, since this is the output:
`hidden: 10    thresh: 0.01    rep: 1/1    steps:   70771	error: 558.04023	time: 58.24 secs`

70K steps? That tells me that it didn't converge that well, so I'm not expecting a great deal of improvement here, if any. But we shall see...

```{r training_output_2}

titanic_training_output_2 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_2$net.result[[1]])

print(titanic_training_output_2)
print(hist(titanic_training_output_2$nn_output))

```
Again, just glancing at this, the results look reasonable—as the output goes from 0 to 1, the actual chance of `Survival` values being 1 goes up as well. And, it's also good that the predicted values (which I guess are more interpretable as probabilities) are clustered around 0 and 1, which means that the model is pretty confident on most of the predictions.

```{r training_accuracy_2}

correct_pct_2<- titanic_training_output_2 %>% mutate(prediction = round(nn_output)) %>%
  filter(as.integer(as.character(Survived)) - prediction == 0) %>%
  nrow() %>% `/`(nrow(titanic_training_output_2))
print(correct_pct_2)

```
It's pretty much the same as before on the training data. Hopefully it's a little less brittle this time on the testing set?

```{r testing_output_2}

make_testing_output <- function(model, testing_data_final, write_name = NULL, dir = "~/repos/toolkit/learning_projects/titanic_nn", nn_output_vec = 2) {
  titanic_testing_output <- predict(model, testing_data_final) %>%
    compile_titanic_output_2(testing_data_final, rep(NA, nrow(testing_data_final)), nn_output = ., output_vec = nn_output_vec) %>% mutate(prediction = round(nn_output))
  # print(titanic_testing_output$nn_output)
  if (!is.null(write_name))
    write_titanic_prediction(prediction = titanic_testing_output$prediction, testing_data = testing_data, name = write_name, dir = dir)
  return(titanic_testing_output)
}

titanic_testing_output_2 <- make_testing_output(titanic_model_2, testing_data_rotated, write_name = "titanic_prediction_nn_2.csv")
```
There's a whopping 46-line difference between this prediction and the previous one, which is more than a 10% difference. I feel that this is either a very good sign, or a very bad one, which makes me very nervous...

Submitting this result, I got what may have been the worst outcome, which was... literally the exact same correctness percentage as before, 0.76315. How this could've happened despite the differences boggles my mind, and I did check that I in fact submitted the right file.

The problem now is that I have no clue as to whether what I did made any sense whatsoever, given that I've obtained no additional information. And the other thing is that in the process of trying this step, I've in fact done 3 of the 4 things I was going to try.

So I suppose there's nothing to do but to attempt the last item—altering the node structure.

## Titanic Prediction via NN (third try)

First, though, I noticed that in the `neuralnet` function, it is possible to narrow down the output to a single node (whereas I had 2 before), so I'm going to try that. I don't think mathematically that should make any difference here, though.

```{r titanic_nn_3}

titanic_model_3 <- neuralnet(formula = Survived == "1" ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10), linear.output = FALSE, err.fct = "ce", lifesign = "full")

# titanic_training_output_3 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_2$net.result[[1]])
# 
# print(titanic_training_output_3)
# print(hist(titanic_training_output_3$nn_output))
# 
# correct_pct_3 <- titanic_training_output_3 %>% mutate(prediction = round(nn_output)) %>%
#   filter(as.integer(as.character(Survived)) - prediction == 0) %>%
#   nrow() %>% `/`(nrow(titanic_training_output_3))
# print(correct_pct_3)
# 
# titanic_testing_output_3 <- make_testing_output(titanic_model_3, testing_data_rotated, write_name = "titanic_prediction_nn_3.csv")

make_titanic_prediction <- function(titanic_model, iteration, output_vec = 2) {
  titanic_training_output <- compile_titanic_output_2(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model$net.result[[1]], output_vec = output_vec)
  
  correct_pct <- titanic_training_output %>% mutate(prediction = round(nn_output)) %>%
    filter(as.integer(as.character(Survived)) - prediction == 0) %>%
    nrow() %>% `/`(nrow(titanic_training_output))
  
  titanic_testing_output <- make_testing_output(titanic_model, testing_data_rotated, write_name = paste0( "titanic_prediction_nn_",iteration,".csv"), nn_output_vec = output_vec)
  
  results <- list(
    titanic_model = titanic_model,
    titanic_training_output = titanic_training_output,
    correct_pct = correct_pct,
    titanic_testing_output = titanic_testing_output
  )
  return(results)
}

titanic_results_3 <- make_titanic_prediction(titanic_model_3, 3)

```

I was wrong; this did make a difference... it made the prediction much worse, down to ~71-73%. Perhaps it's restricting the error minimization to only the rows where `Survived == 1`? I'll probably look into that later, but certainly that seems like a trap.

## Titanic Prediction via NN (fourth try)

Back to changing the node structure. I'm going to just plug in some random node configurations and see what happens.

```{r titanic_nn_4}

titanic_model_4 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 3), linear.output = FALSE, err.fct = "ce", lifesign = "full")

```

I tried a few different configurations, but it seems that every time I increase the number of hidden layers, I run into the problem of the gradient not converging fast enough within `stepmax`. Going to do some research and play around with it some more; perhaps it's a learning rate problem?

(A few hours later...)

Some very interesting results from the experimentation, though nothing definitive. First, a list of some of the things I tried (not in any particular order):

```{r titanic_nn_4_experiments}

titanic_model_4$a <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 3), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 1e6)

titanic_model_4$b <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 3), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 1e6) # reached 0.0235674344878095

titanic_model_4$c <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 3), linear.output = FALSE, err.fct = "ce", algorithm = "rprop-", learningrate.factor = list(minus = 0.2, plus = 1.2), lifesign = "full", stepmax = 1e6)

titanic_model_4$d <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 3), linear.output = FALSE, err.fct = "ce", algorithm = "rprop-", learningrate.factor = list(minus = 0.1, plus = 1.2), lifesign = "full", stepmax = 1e6)

titanic_model_4$e <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 3), linear.output = FALSE, err.fct = "ce", algorithm = "sag", learningrate.factor = list(minus = 0.2, plus = 1.2), lifesign = "full", stepmax = 1e6)

titanic_model_4$f <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 2), linear.output = FALSE, err.fct = "ce", algorithm = "backprop", learningrate = 0.0001, lifesign = "full", stepmax = 2e5)

titanic_model_4$g <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 2), linear.output = FALSE, err.fct = "ce", algorithm = "backprop", learningrate = 0.0001, lifesign = "full", stepmax = 2e5)

titanic_model_4$h <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(4, 2), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 2e5)

titanic_model_4$i <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(3, 2), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 2e5)

titanic_model_4$j <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 2), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 2e5)

titanic_model_4$k <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 2), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 2e5)

titanic_model_4$l <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 3), linear.output = FALSE, err.fct = "ce", learningrate.factor = list(minus = 0.2, plus = 1.2), lifesign = "full", stepmax = 2e5)

titanic_model_4$m <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10, 2), linear.output = FALSE, err.fct = "ce", learningrate.factor = list(minus = 0.2, plus = 1.2), lifesign = "full", stepmax = 2e5)

titanic_model_4$n <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 3), linear.output = FALSE, err.fct = "ce", algorithm = "backprop", learningrate = .02, lifesign = "full", stepmax = 2e5)

```

So it definitely feels like I'm largely stabbing in the dark here. At the moment, I don't think I have enough of an understanding of the underlying math behind the backprop algorithms to know why they behave the way they do.

With that said, here are some observations and questions from these runs:
1. Practically speaking, it doesn't seem like there's a guarantee that the partial derivatives of the loss function will converge to below any arbitrary threshold, at least within the max steps I limited myself to, ranged between 100K and 1M steps. The default threshold that `neuralnet` sets for training completion is .01, which I couldn't reach at all in the vast majority of tries. A lot of times the partial values get "stuck" at a certain point and just don't go down further for a long time. Note: the "value" of the partial derivatives that `neuralnet` checks against the `threshold` parameter is the max absolute value of all of the partials.
2. I wonder about the reason for the non-convergence. My theory right now is that because the size of the dataset is pretty small (~900 observations), the stochastic gradient estimation has high variance, since the batches the estimates are being extracted from are also getting small; perhaps this effect (if it's real) becomes worse the smaller the partials get.
3. It does seem that, at least for some hyperparameter settings, the partials _do_ keep going down as more steps are pumped into the training process. For run b, for example, they continued to decrease all the way until about 800K steps, and might have continued to do so if the run had gone on for longer.
4. It's also interesting that the "stuck" points tend to differ somewhat between runs—I've seen differences of 2-3x, or even more. This may very well be due to the random starting weights that `neuralnet` applies, though maybe it's also support for the theory in #2.
5. Convergence seems to be more difficult when there are more hidden layers, even if the number of connections between layers remain the same—for example, the 5/2 hidden layer structure seems to converge much more slowly than just a single 10-node hidden layer, though increasing the number of total nodes also has the same effect. Being completely speculative, my guess would be that having more layers makes the loss function more nonlinear (since the activation function is being applied more times), and that in turn makes the local gradient structure more volatile.
6. I tried most of the backprop algorithms available in the package, but as far as I could tell, the default `rprop+` worked at least as well as anything else. I also tried adjusting some of the other hyperparameters such as learning rate, but it's pretty hard to tell what effect they have since `neuralnet` doesn't provide any tools for exposing the internals of the training process. Indeed, I'm finding this package to be pretty constricting when it comes to experimentation, so I may try a different neural net package in future work.

I think I'm getting to the end of my useful time with this dataset, but for the sake of completion, I'll train a 5/2 network and see how it does on the prediction front. I suspect it won't be any better than before—it feels like perhaps that NN fare better when it comes to larger datasets with highly nonlinear relationships, which this dataset doesn't seem to be.

```{r titanic_nn_4_predictions}

titanic_model_4$o <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 2), linear.output = FALSE, err.fct = "ce", lifesign = "full", stepmax = 2e6, threshold = .01)

titanic_results_4 <- make_titanic_prediction(titanic_model_4$o, 4)
```

And appropriately enough, the success rate turned out to be... 77.99%, which is the exact same rate as the initial model I trained for Titanic. Narrative symmetry aside, that _is_ the best performance of any of the NN models I've trained so far, so if it's not a fluke from the stochastic process, it certainly speaks well for the multi-hidden-layer architecture compared to the single-hidden-layer one earlier.

I will say, though, that the NN seems less robust than linear models—despite its performance on the test set being somewhere around 75-78%, on the training dataset it was pretty much always somewhere around 85%. That actually makes a lot of intuitive sense to me, since the universal approximation theorem means that in theory, you can fit a NN to as close to the target variable as you want, as long as you can keep adding more nodes and pump more compute into it. That sounds amazing, but it's probably also a recipe for overfitting, especially for smaller datasets. I suspect that having clean and representative data is even more important for neural networks than it is for other models.

In any case, that's the end of the work on this dataset.

## Some Additional Thoughts

There's obviously far more to learn about neural nets, but here are my initial thoughts after this first exposure to it, from both my experiences here and from reading. I reiterate the caveat that these may not turn out to be correct—they're just my current understanding:
1. Compared to more linear models, neural nets take a lot more computation to fit for the same datasets. Especially for smaller datasets or simple problems, they might be overkill.
2. Despite their relatively straightforward mathematical underpinnings, in practice neural nets function like black boxes (or maybe very dark gray boxes).
3. Neural nets are better at solving nonlinear mapping problems such as image recognition or playing games compared to other machine learning techniques.
4. Neural nets scale well with additional compute.
