# Neural Net Exploration

## Introduction

This project will be an exploration of machine learning using neural nets. I'll be using the same Titanic dataset I used for the other exploration project in `titanic/titanic.Rmd`, starting with the post-PCA transform version. Even more than that other one, I'll be taking an experimental approach here, recording all the things I tried, including the mistakes and false paths I go down. My hope is that this will serve as a chronicle of someone's initial foray into this topic.

Please note that this is not the work of an expert, and any conclusions and procedures shown here may be incorrect.

## Overall Expectations

I'd like to start off with what my expectations on what I will find out with this exploration. From what I've read so far, compared to the individual linear models often used in machine learning, neural nets would seem to be applicable to a wider range of problems, but are both more computationally expensive and more of a black box.

I think the compute requirement has turned out to be a strength rather than a weakness (at least for complex problems), because the flip side of it is that (deep) neural nets also scale way better with additional compute, which we are continually getting more of. Being a black box is a much bigger problem, because it greatly hinders the interpretability of the system, and makes it way harder to fix alignment problems that will inevitably pop up.

With regards to my specific effort on the Titanic dataset, I have my doubts as to how much better the neural net can be compared to the previous model I tried (KNN). I think realistically, there's a limit to how predictive the information we're given can truly be for the survival—surely there are factors outside of what's recorded in the dataset that contributed to whether someone survived or not. With that said, maybe there is something that the neural net can pick up that the other model couldn't; I would be quite happy if it got to 90% accuracy on the testing set.

## Initial Prep

We'll prep the data the same way we did earlier.

```{r data_prep}

pacman::p_load(caret, dataPreparation, magrittr, dplyr)
set.seed(3000)

# recreating data prep from titanic.Rmd
source("~/repos/toolkit/learning_projects/titanic/titanic_functions.R")
training_data <- read.csv("~/repos/toolkit/datasets/titanic/train.csv")
testing_data <- read.csv("~/repos/toolkit/datasets/titanic/test.csv")
training_data_prepped <- prep_training_data(training_data)
training_data_pca <- pca_titanic_data(training_data_prepped)
training_data_rotated <- rotate_training_data(training_data_prepped, training_data_pca)

training_data_cleaned <- clean_titanic_data(training_data)
training_data_encoded <- encode_titanic_data(training_data_cleaned)
testing_data_prepped <- prep_testing_data_2(testing_data, training_data_encoded)
testing_data_rotated <- rotate_testing_data(testing_data_prepped, training_data_pca)

```

## Initial Test

Before trying the Titanic dataset, I want to do an initial test on a very simple example, one that I expect the neural net to have no trouble with even with largely default settings: predicting the roots of some positive numbers.

```{r roots_test}

library("neuralnet")
training_data <- read.csv("~/repos/toolkit/datasets/roots.csv", header=TRUE)
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

It's... not horrible? At least it follows the shape of the expected values. But it's not great either. The reason is almost certainly that the default number of neurons is just 1. So, if we increase that...
```{r}
simple_test_model <- neuralnet(formula = root_k ~ k, data = training_data, hidden = 10)
output <- cbind(training_data, nn_output = unlist(simple_test_model$net.result))
print(output)
```

Much better. Now let's see how well it interpolates and extrapolates; I expect the standard results for that—interpolation results to be largely comparable to the training set, and extrapolation to be okay at first and get worse as it gets further away.
```{r roots_test_interpolation}

interpolation_test_data <- 1:49 + .5
extrapolation_test_data <- 51:100

predict_root <- function(k_vec, model) {
  data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
    mutate(nn_output = predict(model, select(., k))[,1]) %>%
    mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
roots_interpolation_outputs <- predict_root(interpolation_test_data, simple_test_model)
print(roots_interpolation_outputs)
```

That looks fine, as expected. For extrapolation:

```{r roots_test_extrapolation}

roots_extrapolation_outputs <- predict_root(extrapolation_test_data, simple_test_model)
print(roots_extrapolation_outputs)
plot(roots_extrapolation_outputs$k, roots_extrapolation_outputs$diff_pct)
```

Again, no surprises. It's interesting that the error percentage goes up essentially linearly as `k` gets larger, which means that the error term must have been `O(k^2)`. It seems like there should be a fairly straightforward mathematical proof of that, though I think I'm going to skip over that derivation to get to the main course here.

## Titanic Prediction via NN (first try)

I don't think this will work too well, but I'm going to first try straight up using the exact training parameters as I did with the previous section, except now with the Titanic dataset. It _shouldn't_ work, because the previous output wasn't for classification, while this one definitely is.

```{r titanic_nn_1}

titanic_model_1 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10)

compile_titanic_output_1 <- function(predictors, expected, nn_output) {
  predictors %>% cbind(expected) %>%
    mutate(nn_output = nn_output[,2])
  # data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
  # mutate(nn_output = predict(model, select(., k))[,1]) %>%
  # mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
titanic_training_output_1 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_1$net.result[[1]])

print(titanic_training_output_1)

compile_titanic_output_2 <- function(predictors, expected, nn_output, output_vec = 2) {
  predictors %>% cbind(expected) %>%
    mutate(nn_output = nn_output[,output_vec])
  # data.frame(k = k_vec, root_k = sqrt(k_vec)) %>% 
  # mutate(nn_output = predict(model, select(., k))[,1]) %>%
  # mutate(diff_pct = abs(nn_output - root_k)/root_k)
}
```

This turned out better than I expected, though I'm fairly certain the loss minimization took way longer than it should have to converge (about 50k steps). The results did indeed end up not being categorical, but they still roughly corresponded to the expected results (i.e. the lower values will tend to correspond to `Survived` being 0, and vice versa for higher values).

Clearly I need to do some research on how to turn this into a classification result, but for the sake of getting some sort of result, I'm simply going to round the result to 0 or 1 and see how it does.

```{r titanic_nn_1_verification}

correct_pct <- titanic_training_output_1 %>% mutate(prediction = round(nn_output)) %>%
  filter(as.integer(as.character(Survived)) - prediction == 0) %>%
  nrow() %>% `/`(nrow(titanic_training_output_1))
print(correct_pct)
```

It's... about 85.86%??? This seems absolutely incredible to me, because I have no clue if the neural net parameters are appropriate for this task, and I basically made up the mapping to the binary result without any consultation as to what the best thing to do there is. And yet we still ended up about 4% better than the previous model.

With that said, there's still a good chance that this model is not robust and flounders on the test set, so we should confirm that.

```{r testing_output_1}

titanic_testing_output_1 <- predict(titanic_model_1, testing_data_rotated) %>%
  compile_titanic_output_1(testing_data_rotated, rep(NA, nrow(testing_data_rotated)), nn_output = .)

write_titanic_prediction(prediction = round(titanic_testing_output_1$nn_output), testing_data = testing_data, name = "titanic_prediction_nn_1.csv", dir = "~/repos/toolkit/learning_projects/titanic_nn")
```

Okay, so we got 76.32%, which is _not_ an improvement. Clearly, the model we trained was quite brittle. I wonder if the test dataset required too much extrapolation, or if the model also isn't that good with interpolation, either? In any case, we'll put the finer adjustments to the side for now, and fix the major issues first.

So, what are those major (potential) issues?
1. We clearly need a better system of mapping from the numerical output to the prediction categories. I'm sure there's a standard way of doing this, so this should just be a quick research stint.
2. We're using a 1/10/1 structure. Perhaps changing it around will give better results?
3. I'm pretty sure the default loss function is mean-squared. That might not be appropriate for a binary classifier.
4. I think by default the `neuralnet` trainer doesn't have an activation function enabled, so maybe we were pretty much just doing linear regression?

Hopefully, by looking into these, I can both make the loss function converge faster, and make the model more robust.

## Titanic Prediction via NN (second try)

Okay, so here I have to make it clear that, even more than before, I am now firmly beyond my own current knowledge on the subject at this point. For the sake of my own record I'm still going to write down all the things I try, but I'm going to be making a lot of missteps here. Tread lightly, readers.

In any case, the first thing I'm going to try is to switch the loss function away from squared errors. The reason is that we're doing a binary classification, which I think SSE isn't optimized for; I believe the right loss function is "cross-entropy", from what I've read. So let's try that...

```{r model_2.1}

titanic_model_2 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10, err.fct = "ce", lifesign = "minimal")

```

...It didn't work. This is the error I got:

`hidden: 10    thresh: 0.01    rep: 1/1    steps: Warning in log(x) : NaNs produced`
`Warning in log(x) : NaNs produced`
`Warning in log(1 - x) : NaNs produced`
`Warning: 'err.fct' does not fit 'data' or 'act.fct'`
`  81034	error: NaN      	time: 59.3 secs`
  
I think I know why; the current output from the model is not actually constrained within 0 and 1, which obviously would cause `NaNs` when passed through the `log` function as part of cross-entropy. And the reason the output is not so constrained is (I think) that we don't have an activation function specified, so the node connection weights are just being passed straight through.

I'm going to try enabling the activation function (which by default is logistical regression).

```{r model_2.2}

titanic_model_2 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = 10, linear.output = FALSE, err.fct = "ce", lifesign = "minimal")
```

It worked! Or at least, there's not an error. On reading the `neuralnet` docs, I notice that I was wrong about the activation function not being enabled—it's more that it just wasn't applied to the final output, which `linear.output = FALSE` enables.

I have a feeling that it didn't actually work that well though, since this is the output:
`hidden: 10    thresh: 0.01    rep: 1/1    steps:   70771	error: 558.04023	time: 58.24 secs`

70K steps? That tells me that it didn't converge that well, so I'm not expecting a great deal of improvement here, if any. But we shall see...

```{r training_output_2}

titanic_training_output_2 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_2$net.result[[1]])

print(titanic_training_output_2)
print(hist(titanic_training_output_2$nn_output))

```
Again, just glancing at this, the results look reasonable—as the output goes from 0 to 1, the actual chance of `Survival` values being 1 goes up as well. And, it's also good that the predicted values (which I guess are more interpretable as probabilities) are clustered around 0 and 1, which means that the model is pretty confident on most of the predictions.

```{r training_accuracy_2}

correct_pct_2<- titanic_training_output_2 %>% mutate(prediction = round(nn_output)) %>%
  filter(as.integer(as.character(Survived)) - prediction == 0) %>%
  nrow() %>% `/`(nrow(titanic_training_output_2))
print(correct_pct_2)

```
It's pretty much the same as before on the training data. Hopefully it's a little less brittle this time on the testing set?

```{r testing_output_2}

make_testing_output <- function(model, testing_data_final, write_name = NULL, dir = "~/repos/toolkit/learning_projects/titanic_nn", nn_output_vec = 2) {
  titanic_testing_output <- predict(model, testing_data_final) %>%
    compile_titanic_output_2(testing_data_final, rep(NA, nrow(testing_data_final)), nn_output = ., output_vec = nn_output_vec) %>% mutate(prediction = round(nn_output))
  # print(titanic_testing_output$nn_output)
  if (!is.null(write_name))
    write_titanic_prediction(prediction = titanic_testing_output$prediction, testing_data = testing_data, name = write_name, dir = dir)
  return(titanic_testing_output)
}

titanic_testing_output_2 <- make_testing_output(titanic_model_2, testing_data_rotated, write_name = "titanic_prediction_nn_2.csv")
```
There's a whopping 46-line difference between this prediction and the previous one, which is more than a 10% difference. I feel that this is either a very good sign, or a very bad one, which makes me very nervous...

Submitting this result, I got what may have been the worst outcome, which was... literally the exact same correctness percentage as before, 0.76315. How this could've happened despite the differences boggles my mind, and I did check that I in fact submitted the right file.

The problem now is that I have no clue as to whether what I did made any sense whatsoever, given that I've obtained no additional information. And the other thing is that in the process of trying this step, I've in fact done 3 of the 4 things I was going to try.

So I suppose there's nothing to do but to attempt the last item—altering the node structure.

## Titanic Prediction via NN (third try)

First, though, I noticed that in the `neuralnet` function, it is possible to narrow down the output to a single node (whereas I had 2 before), so I'm going to try that. I don't think mathematically that should make any difference here, though.

```{r titanic_nn_3}

titanic_model_3 <- neuralnet(formula = Survived == "1" ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(10), linear.output = FALSE, err.fct = "ce", lifesign = "full")

# titanic_training_output_3 <- compile_titanic_output_1(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model_2$net.result[[1]])
# 
# print(titanic_training_output_3)
# print(hist(titanic_training_output_3$nn_output))
# 
# correct_pct_3 <- titanic_training_output_3 %>% mutate(prediction = round(nn_output)) %>%
#   filter(as.integer(as.character(Survived)) - prediction == 0) %>%
#   nrow() %>% `/`(nrow(titanic_training_output_3))
# print(correct_pct_3)
# 
# titanic_testing_output_3 <- make_testing_output(titanic_model_3, testing_data_rotated, write_name = "titanic_prediction_nn_3.csv")

make_titanic_prediction <- function(titanic_model, iteration, output_vec = 2) {
  titanic_training_output <- compile_titanic_output_2(select(training_data_rotated, -Survived), select(training_data_rotated, Survived), titanic_model$net.result[[1]], output_vec = output_vec)
  
  correct_pct <- titanic_training_output %>% mutate(prediction = round(nn_output)) %>%
    filter(as.integer(as.character(Survived)) - prediction == 0) %>%
    nrow() %>% `/`(nrow(titanic_training_output))
  
  titanic_testing_output <- make_testing_output(titanic_model, testing_data_rotated, write_name = paste0( "titanic_prediction_nn_",iteration,".csv"), nn_output_vec = output_vec)
  
  results <- list(
    titanic_model = titanic_model,
    titanic_training_output = titanic_training_output,
    correct_pct = correct_pct,
    titanic_testing_output = titanic_testing_output
  )
  return(results)
}

titanic_results_3 <- make_titanic_prediction(titanic_model_3, 3)

```

I was wrong; this did make a difference... it made the prediction much worse, down to ~71-73%. Perhaps it's restricting the error minimization to only the rows where `Survived == 1`? I'll probably look into that later, but certainly that seems like a trap.

## Titanic Prediction via NN (fourth try)

Back to changing the node structure. I'm going to just plug in some random node configurations and see what happens.

```{r titanic_nn_4}

titanic_model_4 <- neuralnet(formula = Survived ~ PC1 + PC2 + PC3 + PC4 + PC5, data = training_data_rotated, hidden = c(5, 3), linear.output = FALSE, err.fct = "ce", lifesign = "full")

```

I tried a few different configurations, but it seems that every time I increase the number of hidden layers, I run into the problem of the gradient not converging fast enough within `stepmax`. Going to do some research and play around with it some more; perhaps it's a learning rate problem?